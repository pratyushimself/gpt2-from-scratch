# ğŸ§  Building a Transformer Model from Scratch (GPT-style)

This project walks through the full pipeline of building and training a GPT-like transformer model from scratch â€” starting with a custom tokenizer, followed by a small transformer architecture, and finally training it on a subset of OpenWebText data.

---

## ğŸ¯ Purpose

The goal of this project is **not just to fine-tune an existing GPT model**, but to **understand and implement the entire process** behind how transformer models like GPT work â€” from tokenization to training to text generation.

This was done using a simplified architecture and smaller dataset, so output quality isnâ€™t the main focus. Instead, the project is meant to help you:

- Learn how GPT-style models are trained from raw data.
- Build and customize your own tokenizer.
- Structure a transformer training workflow using PyTorch and HuggingFace.

---

## ğŸ” Key Concepts

- **Custom Tokenizer**: Built using Byte-Pair Encoding (BPE) from raw text data.
- **Model Architecture**: A compact GPT-2-style model with 4 layers and 256-dimensional embeddings.
- **Training**: Performed chunk-by-chunk on tokenized data (~20 `.pt` chunks for demo).
- **Resume Support**: Training can resume from checkpoints (e.g., `checkpoint-56000`).
- **Generation**: Output is generated using the trained model and tokenizer.

---

## ğŸ§ª Data & Personalization

> âš ï¸ _If you're wondering why the generated text might feel generic... it's all in the data._

To make the model generate meaningful, context-aware outputs, **the dataset should reflect the kind of content you want the model to generate**. That could be:

- Your own writing (e.g., journals, text messages, blogs).
- Conversational data in your voice.
- Structured knowledge or domain-specific content.

This version was trained on a small chunked subset of OpenWebText â€” **just 20 training files out of a larger dataset**, for demonstration purposes.

**Bigger and more authentic data leads to better results.**

---
## ğŸ› ï¸ Project Structure

- `gpt2_custom_model/` â€” Trained model & checkpoints  
- `tokenizer_gpt2_custom/` â€” Custom tokenizer files  
- `token_chunks/` â€” Training chunks (only 20 used here)  
- `train_from_chunks.ipynb` â€” Full training notebook  
- `train_tokenizer.ipynb` â€” BPE tokenizer training  
- `test_custom_GPT2.ipynb` â€” Testing the model  
- `generate_text.py` â€” Script for text generation  


## ğŸš€ How to Run

To generate text with the trained model:

```bash
python generate_text.py --prompt "Once upon a time" --max_length 100  



