# gpt2-from-scratch
Train a GPT-2 language model from scratch using custom tokenization and chunked OpenWebText data on CPU.
