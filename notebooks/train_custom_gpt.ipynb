{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46430200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Adding: tokenized_files\\tokenized_1.txt\n",
      "üìÑ Adding: tokenized_files\\tokenized_2.txt\n",
      "üìÑ Adding: tokenized_files\\tokenized_3.txt\n",
      "üìÑ Adding: tokenized_files\\tokenized_4.txt\n",
      "‚úÖ Merged tokenized file created: merged_tokenized.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    GPT2Config,\n",
    "    GPT2LMHeadModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Step 1: Setup Paths ===\n",
    "chunk_dir = \"token_chunks\"\n",
    "model_output_dir = \"gpt2_custom_model\"\n",
    "block_size = 512\n",
    "vocab_size = 30000\n",
    "resume_checkpoint = None  # Set to something like \"gpt2_custom_model/checkpoint-56000\" if resuming\n",
    "\n",
    "# === Step 2: Dataset Loader ===\n",
    "class ChunkDataset(Dataset):\n",
    "    def __init__(self, token_ids, block_size=512):\n",
    "        self.examples = [token_ids[i:i+block_size] for i in range(0, len(token_ids) - block_size, block_size)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.examples[idx])\n",
    "        return {\"input_ids\": x, \"labels\": x.clone()}\n",
    "\n",
    "# === Step 3: Model Config ===\n",
    "config = GPT2Config(\n",
    "    vocab_size=vocab_size,\n",
    "    n_positions=512,\n",
    "    n_ctx=512,\n",
    "    n_embd=256,\n",
    "    n_layer=4,\n",
    "    n_head=4,\n",
    ")\n",
    "model = GPT2LMHeadModel(config)\n",
    "\n",
    "# === Step 4: Training Args ===\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=1,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    prediction_loss_only=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# === Step 5: Trainer Setup ===\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=None, mlm=False)\n",
    "trainer = Trainer(model=model, args=training_args, data_collator=data_collator)\n",
    "\n",
    "# === Step 6: Train on First 20 Chunks ===\n",
    "chunk_files = sorted([f for f in os.listdir(chunk_dir) if f.endswith(\".pt\")])[:20]\n",
    "print(f\"üß† Training on {len(chunk_files)} chunks{' from checkpoint...' if resume_checkpoint else ''}\")\n",
    "\n",
    "if resume_checkpoint:\n",
    "    print(f\"üîÅ Resuming from checkpoint: {resume_checkpoint}\")\n",
    "\n",
    "for i, file in enumerate(chunk_files):\n",
    "    file_path = os.path.join(chunk_dir, file)\n",
    "    print(f\"\\nüì¶ Loading chunk {i+1}/{len(chunk_files)}: {file}\")\n",
    "    token_ids = torch.load(file_path)\n",
    "    dataset = ChunkDataset(token_ids, block_size)\n",
    "    trainer.train_dataset = dataset\n",
    "\n",
    "    print(f\"üöÄ Training on chunk {i+1} with {len(dataset)} samples...\")\n",
    "    trainer.train(resume_from_checkpoint=resume_checkpoint if i == 0 else None)\n",
    "    print(f\"‚úÖ Finished training chunk {i+1}\")\n",
    "\n",
    "# === Step 7: Save Model ===\n",
    "trainer.save_model(model_output_dir)\n",
    "print(f\"\\n‚úÖ Training complete! Model saved to: {model_output_dir}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
