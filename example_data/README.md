# Example Data (Not Included)

Due to size constraints, the actual CSV files and tokenized chunks are not uploaded.

ğŸ“ To recreate them, follow:
- `1_train_tokenizer.ipynb` â†’ Create tokenizer
- `2_train_custom_gpt2.ipynb` â†’ Load your large dataset, split, tokenize, and chunk it

ğŸ§ª For testing, you can use your own small `.csv` file containing plain text.

