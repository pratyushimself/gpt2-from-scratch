# Example Data (Not Included)

Due to size constraints, the actual CSV files and tokenized chunks are not uploaded.

📝 To recreate them, follow:
- `1_train_tokenizer.ipynb` → Create tokenizer
- `2_train_custom_gpt2.ipynb` → Load your large dataset, split, tokenize, and chunk it

🧪 For testing, you can use your own small `.csv` file containing plain text.

